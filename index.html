
<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 3.2.0">
  <meta name="generator" content="Hugo 0.53" />
  <meta name="author" content="Eldar Insafutdinov">

  
  
  
  
    
  
  <meta name="description" content="">

  
  <link rel="alternate" hreflang="en-us" href="/">

  


  

  

  

  

  

  

  
  
  
  <meta name="theme-color" content="#3f51b5">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha256-eSi1q2PG6J7g7ib17yAaWMcrr5GrtohYChqibrV7PBE=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" crossorigin="anonymous">
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono">
  

  <link rel="stylesheet" href="/styles.css">
  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-131578234-1', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Eldar Insafutdinov">
  <link rel="feed" href="/index.xml" type="application/rss+xml" title="Eldar Insafutdinov">
  

  <link rel="manifest" href="/site.webmanifest">
  
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/">

  
  
  
  
    
    
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Eldar Insafutdinov">
  <meta property="og:url" content="/">
  <meta property="og:title" content="Eldar Insafutdinov">
  <meta property="og:description" content=""><meta property="og:image" content="/img/portrait.jpg">
  <meta property="og:locale" content="en-us">
  
  <meta property="og:updated_time" content="2017-10-15T00:00:00&#43;01:00">
  

  

  

  <title>Eldar Insafutdinov</title>

</head>
<body id="top" data-spy="scroll" data-target="#navbar-main" data-offset="71" >
  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>





<span id="homepage" style="display: none"></span>


  





  
  
  
  <section id="about" class="home-section">
    <div class="container">
      



<div class="row" itemprop="author" itemscope itemtype="http://schema.org/Person" itemref="person-email person-telephone person-address">

  <div class="col-12 col-lg-8" itemprop="description">

    <div class="portrait-title">
      <h1 itemprop="name">Eldar Insafutdinov</h1>
    </div>
    <p>I am a postdoctoral researcher in the <a href="https://www.robots.ox.ac.uk/~vgg/" target="_blank">Visual Geometry Group</a> at the University of Oxford working with <a href="https://www.robots.ox.ac.uk/~vedaldi/" target="_blank">Prof. Andrea Vedaldi</a> and <a href="https://eng.ox.ac.uk/people/joao-henriques/" target="_blank">Dr. Joao Henriques</a>. My primary research interest is in self-supervised learning of visual representations for novel view synthesis and acquisition of 3D assets from raw in-the-wild data. I completed my PhD student at the Max Planck Institute for Informatics under the supervision of <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele" target="_blank">Prof. Bernt Schiele</a> working on human pose estimation. I had the pleasure to do an internship at the Intelligent Systems Lab at Intel working with Alexey Dosovitskiy. I obtained my master&rsquo;s degree in Visual Computing from the Saarland University.</p>

<p><a href="mailto:e.insafutdinov@gmail.com" target="_blank">Contact</a> |
<a href="https://scholar.google.co.uk/citations?user=u4unGhAAAAAJ" target="_blank">Google Scholar</a> |
<a href="https://github.com/eldar" target="_blank">GitHub</a> |
<a href="https://linkedin.com/in/eldar-insafutdinov-18845315" target="_blank">LinkedIn</a></p>


    <div class="row">

      

      

    </div>
  </div>

  <div class="col-12 col-lg-4">
    <div id="profile">

      
      <img class="portrait" src="/img/portrait.jpg" itemprop="image" alt="Avatar">
      

      

      <link itemprop="url" href="/">

      <ul class="network-icon" aria-hidden="true">
        
      </ul>

    </div>
  </div>

</div>

    </div>
  </section>
  

  
  
  
  <section id="publications" class="home-section">
    <div class="container">
      




<div>
    <div class="section-heading">
        <h1>Publications</h1>
    </div>
    
    
    

    
      
    

    

    
    
        <div class="row">
    
    
        
    
    <div class="col-12 col-lg-3">
        <div class="ml-3">
            
            
            
            <a href="https://arxiv.org/pdf/2406.04343">
                <img src="/publication/szymanowicz-2025-3dv/featured.gif" itemprop="image" class="paper-teaser">
            </a>
            
        </div>
    </div>
    <div class="col-12 col-lg-9">
        <div class="media stream-item" itemscope itemtype="http://schema.org/ScholarlyArticle">
        <div class="media-body">

            <h3 class="article-title mb-0 mt-0" itemprop="name">
            <a href="https://arxiv.org/pdf/2406.04343" itemprop="url">Flash3D: Feed-Forward Generalisable 3D Scene Reconstruction from a Single Image</a>
            </h3>

            <div class="stream-meta">
                <div itemprop="author">
                    
                    
                    Stanislaw Szymanowicz,
                    
                    
                    
                    <b>Eldar Insafutdinov</b>,
                    
                    
                    
                    Chuanxia Zheng,
                    
                    
                    
                    Dylan Campbell,
                    
                    
                    
                    João F. Henriques,
                    
                    
                    
                    Christian Rupprecht,
                    
                    

                    
                    
                    Andrea Vedaldi
                    
                    
                </div>
            </div>

            
            <div>
                <i>to appear in 3DV, 2025</i>
            </div>
            

            <div class="btn-links">
            







  
    
  


<a class="btn btn-outline-primary my-1 mr-1" href="https://arxiv.org/pdf/2406.04343" target="_blank" rel="noopener">
  pdf
</a>




<a class="btn btn-outline-primary my-1 mr-1" href="https://www.robots.ox.ac.uk/~vgg/research/flash3d/" target="_blank" rel="noopener">
  project page
</a>




  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1" href="javascript:toggleblock('szymanowicz20253dv_abstract')">
  abstract
</a>




<button type="button" class="btn btn-outline-primary my-1 mr-1 js-cite-modal"
        data-filename="/publication/szymanowicz-2025-3dv/szymanowicz25_3dv.bib">
  bibtex
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1" href="https://github.com/eldar/flash3d" target="_blank" rel="noopener">
  code
</a>












            </div>

            
            
            
            
            
            
            <div class="article-style abstract-block" itemprop="articleBody" id="szymanowicz20253dv_abstract" style="display: none;">
            In this paper, we propose Flash3D, a method for scene reconstruction and novel view synthesis from a single image which is both very generalisable and efficient. For generalisability, we start from a &ldquo;foundation&rdquo; model for monocular depth estimation and extend it to a full 3D shape and appearance reconstructor. For efficiency, we base this extension on feed-forward Gaussian Splatting. Specifically, we predict a first layer of 3D Gaussians at the predicted depth, and then add additional layers of Gaussians that are offset in space, allowing the model to complete the reconstruction behind occlusions and truncations. Flash3D is very efficient, trainable on a single GPU in a day, and thus accessible to most researchers. It achieves state-of-the-art results when trained and tested on RealEstate10k. When transferred to unseen datasets like NYU it outperforms competitors by a large margin. More impressively, when transferred to KITTI, Flash3D achieves better PSNR than methods trained specifically on that dataset. In some instances, it even outperforms recent methods that use multiple views as input.
            </div>
            

        </div>
        </div>
    </div>
</div>
    
        <div class="row">
    
    
        
    
    <div class="col-12 col-lg-3">
        <div class="ml-3">
            
            
            
            <a href="https://arxiv.org/pdf/2206.06340">
                <img src="/publication/insafutdinov-2022-eccv/featured.gif" itemprop="image" class="paper-teaser">
            </a>
            
        </div>
    </div>
    <div class="col-12 col-lg-9">
        <div class="media stream-item" itemscope itemtype="http://schema.org/ScholarlyArticle">
        <div class="media-body">

            <h3 class="article-title mb-0 mt-0" itemprop="name">
            <a href="https://arxiv.org/pdf/2206.06340" itemprop="url">SNeS: Learning Probably Symmetric Neural Surfaces from Incomplete Data</a>
            </h3>

            <div class="stream-meta">
                <div itemprop="author">
                    
                    
                    <b>Eldar Insafutdinov</b>,
                    
                    
                    
                    Dylan Campbell,
                    
                    
                    
                    João F. Henriques,
                    
                    

                    
                    
                    Andrea Vedaldi
                    
                    
                </div>
            </div>

            
            <div>
                <i>ECCV, 2022</i>
            </div>
            

            <div class="btn-links">
            







  
    
  


<a class="btn btn-outline-primary my-1 mr-1" href="https://arxiv.org/pdf/2206.06340" target="_blank" rel="noopener">
  pdf
</a>




<a class="btn btn-outline-primary my-1 mr-1" href="https://www.robots.ox.ac.uk/~vgg/research/snes/" target="_blank" rel="noopener">
  project page
</a>




  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1" href="javascript:toggleblock('insafutdinov2022eccv_abstract')">
  abstract
</a>




<button type="button" class="btn btn-outline-primary my-1 mr-1 js-cite-modal"
        data-filename="/publication/insafutdinov-2022-eccv/insafutdinov-22-snes.bib">
  bibtex
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1" href="https://github.com/eldar/snes" target="_blank" rel="noopener">
  code
</a>












            </div>

            
            
            
            
            
            
            <div class="article-style abstract-block" itemprop="articleBody" id="insafutdinov2022eccv_abstract" style="display: none;">
            We present a method for the accurate 3D reconstruction of partly-symmetric objects. We build on the strengths of recent advances in neural reconstruction and rendering such as Neural Radiance Fields (NeRF). A major shortcoming of such approaches is that they fail to reconstruct any part of the object which is not clearly visible in the training image, which is often the case for in-the-wild images and videos. When evidence is lacking, structural priors such as symmetry can be used to complete the missing information. However, exploiting such priors in neural rendering is highly non-trivial: while geometry and non-reflective materials may be symmetric, shadows and reflections from the ambient scene are not symmetric in general. To address this, we apply a soft symmetry constraint to the 3D geometry and material properties, having factored appearance into lighting, albedo colour and reflectivity. We evaluate our method on the recently introduced CO3D dataset, focusing on the car category due to the challenge of reconstructing highly-reflective materials. We show that it can reconstruct unobserved regions with high fidelity and render high-quality novel view images.
            </div>
            

        </div>
        </div>
    </div>
</div>
    
        <div class="row">
    
    
        
    
    <div class="col-12 col-lg-3">
        <div class="ml-3">
            
            
            
            <a href="https://www.robots.ox.ac.uk/~vedaldi/assets/pubs/mccraith22lifting.pdf">
                <img src="/publication/mccraith-2022-icra/featured.png" itemprop="image" class="paper-teaser">
            </a>
            
        </div>
    </div>
    <div class="col-12 col-lg-9">
        <div class="media stream-item" itemscope itemtype="http://schema.org/ScholarlyArticle">
        <div class="media-body">

            <h3 class="article-title mb-0 mt-0" itemprop="name">
            <a href="https://www.robots.ox.ac.uk/~vedaldi/assets/pubs/mccraith22lifting.pdf" itemprop="url">Lifting 2D Object Locations to 3D by Discounting LiDAR Outliers across Objects and Views</a>
            </h3>

            <div class="stream-meta">
                <div itemprop="author">
                    
                    
                    Robert McCraith,
                    
                    
                    
                    <b>Eldar Insafutdinov</b>,
                    
                    
                    
                    Lukas Neumann,
                    
                    

                    
                    
                    Andrea Vedaldi
                    
                    
                </div>
            </div>

            
            <div>
                <i>ICRA, 2022</i>
            </div>
            

            <div class="btn-links">
            







  
    
  


<a class="btn btn-outline-primary my-1 mr-1" href="https://www.robots.ox.ac.uk/~vedaldi/assets/pubs/mccraith22lifting.pdf" target="_blank" rel="noopener">
  pdf
</a>







  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1" href="javascript:toggleblock('mccraith2022icra_abstract')">
  abstract
</a>




<button type="button" class="btn btn-outline-primary my-1 mr-1 js-cite-modal"
        data-filename="/publication/mccraith-2022-icra/mccraith2022icra.bib">
  bibtex
</button>













            </div>

            
            
            
            
            
            
            <div class="article-style abstract-block" itemprop="articleBody" id="mccraith2022icra_abstract" style="display: none;">
            We present a system for automatic converting of 2D mask object predictions and raw LiDAR point clouds into full 3D bounding boxes of objects. Because the LiDAR point clouds are partial, directly fitting bounding boxes to the point clouds is meaningless. Instead, we suggest that obtaining good results requires sharing information between all objects in the dataset jointly, over multiple frames. We then make three improvements to the baseline. First, we address ambiguities in predicting the object rotations via direct optimization in this space while still backpropagating rotation prediction through the model. Second, we explicitly model outliers and task the network with learning their typical patterns, thus better discounting them. Third, we enforce temporal consistency when video data is available. With these contributions, our method significantly outperforms previous work despite the fact that those methods use significantly more complex pipelines, 3D models and additional human-annotated external sources of prior information.
            </div>
            

        </div>
        </div>
    </div>
</div>
    
        <div class="row">
    
    
        
    
    <div class="col-12 col-lg-3">
        <div class="ml-3">
            
            
            
            <a href="https://arxiv.org/pdf/1908.07117">
                <img src="/publication/lazova-2019-3dv/featured.gif" itemprop="image" class="paper-teaser">
            </a>
            
        </div>
    </div>
    <div class="col-12 col-lg-9">
        <div class="media stream-item" itemscope itemtype="http://schema.org/ScholarlyArticle">
        <div class="media-body">

            <h3 class="article-title mb-0 mt-0" itemprop="name">
            <a href="https://arxiv.org/pdf/1908.07117" itemprop="url">360-Degree Textures of People in Clothing from a Single Image</a>
            </h3>

            <div class="stream-meta">
                <div itemprop="author">
                    
                    
                    Verica Lazova,
                    
                    
                    
                    <b>Eldar Insafutdinov</b>,
                    
                    

                    
                    
                    Verica Lazova
                    
                    
                </div>
            </div>

            
            <div>
                <i>3DV, 2019</i>
            </div>
            

            <div class="btn-links">
            







  
    
  


<a class="btn btn-outline-primary my-1 mr-1" href="https://arxiv.org/pdf/1908.07117" target="_blank" rel="noopener">
  pdf
</a>




<a class="btn btn-outline-primary my-1 mr-1" href="https://virtualhumans.mpi-inf.mpg.de/360tex/" target="_blank" rel="noopener">
  project page
</a>




  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1" href="javascript:toggleblock('lazova20193dv_abstract')">
  abstract
</a>




<button type="button" class="btn btn-outline-primary my-1 mr-1 js-cite-modal"
        data-filename="/publication/lazova-2019-3dv/lazova-19-avatars.bib">
  bibtex
</button>













            </div>

            
            
            
            
            
            
            <div class="article-style abstract-block" itemprop="articleBody" id="lazova20193dv_abstract" style="display: none;">
            In this paper we predict a full 3D avatar of a person from a single image. We infer texture and geometry in the UV-space of the SMPL model using an image-to-image translation method. Given partial texture and segmentation layout maps derived from the input view, our model predicts the complete segmentation map, the complete texture map, and a displacement map. The predicted maps can be applied to the SMPL model in order to naturally generalize to novel poses, shapes, and even new clothing. In order to learn our model in a common UV-space, we non-rigidly register the SMPL model to thousands of 3D scans, effectively encoding textures and geometries as images in correspondence. This turns a difficult 3D inference task into a simpler image-to-image translation one. Results on rendered scans of people and images from the DeepFashion dataset demonstrate that our method can reconstruct plausible 3D avatars from a single image. We further use our model to digitally change pose, shape, swap garments between people and edit clothing. To encourage research in this direction we will make the source code available for research purpose.
            </div>
            

        </div>
        </div>
    </div>
</div>
    
        <div class="row">
    
    
        
    
    <div class="col-12 col-lg-3">
        <div class="ml-3">
            
            
            
            <a href="https://arxiv.org/pdf/1810.09381">
                <img src="/publication/insafutdinov-2018-neurips/featured.gif" itemprop="image" class="paper-teaser">
            </a>
            
        </div>
    </div>
    <div class="col-12 col-lg-9">
        <div class="media stream-item" itemscope itemtype="http://schema.org/ScholarlyArticle">
        <div class="media-body">

            <h3 class="article-title mb-0 mt-0" itemprop="name">
            <a href="https://arxiv.org/pdf/1810.09381" itemprop="url">Unsupervised Learning of Shape and Pose with Differentiable Point Clouds</a>
            </h3>

            <div class="stream-meta">
                <div itemprop="author">
                    
                    
                    <b>Eldar Insafutdinov</b>,
                    
                    

                    
                    
                    Alexey Dosovitskiy
                    
                    
                </div>
            </div>

            
            <div>
                <i>NeurIPS, 2018</i>
            </div>
            

            <div class="btn-links">
            







  
    
  


<a class="btn btn-outline-primary my-1 mr-1" href="https://arxiv.org/pdf/1810.09381" target="_blank" rel="noopener">
  pdf
</a>




<a class="btn btn-outline-primary my-1 mr-1" href="https://eldar.insafutdinov.com/PointClouds/" target="_blank" rel="noopener">
  project page
</a>




  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1" href="javascript:toggleblock('insafutdinov2018neurips_abstract')">
  abstract
</a>




<button type="button" class="btn btn-outline-primary my-1 mr-1 js-cite-modal"
        data-filename="/publication/insafutdinov-2018-neurips/insafutdinov-18-pointclouds.bib">
  bibtex
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1" href="https://github.com/eldar/differentiable-point-clouds" target="_blank" rel="noopener">
  code
</a>








  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1" href="https://www.youtube.com/watch?v=LuIGovKeo60" target="_blank" rel="noopener">
  video
</a>





            </div>

            
            
            
            
            
            
            <div class="article-style abstract-block" itemprop="articleBody" id="insafutdinov2018neurips_abstract" style="display: none;">
            We address the problem of learning accurate 3D shape and camera pose from a collection of unlabeled category-specific images. We train a convolutional network to predict both the shape and the pose from a single image by minimizing the reprojection error: given several views of an object, the projections of the predicted shapes to the predicted camera poses should match the provided views. To deal with pose ambiguity, we introduce an ensemble of pose predictors that we then distill it to a single “student” model. To allow for efficient learning of high-fidelity shapes, we represent the shapes by point clouds and devise a formulation allowing for differentiable projection of these. Our experiments show that the distilled ensemble of pose predictors learns to estimate the pose accurately, while the point cloud representation allows to predict detailed shape models.
            </div>
            

        </div>
        </div>
    </div>
</div>
    
        <div class="row">
    
    
        
    
    <div class="col-12 col-lg-3">
        <div class="ml-3">
            
            
            
            <a href="https://arxiv.org/pdf/1710.10000">
                <img src="/publication/andriluka-2018-cvpr/featured.gif" itemprop="image" class="paper-teaser">
            </a>
            
        </div>
    </div>
    <div class="col-12 col-lg-9">
        <div class="media stream-item" itemscope itemtype="http://schema.org/ScholarlyArticle">
        <div class="media-body">

            <h3 class="article-title mb-0 mt-0" itemprop="name">
            <a href="https://arxiv.org/pdf/1710.10000" itemprop="url">PoseTrack: A Benchmark for Human Pose Estimation and Tracking</a>
            </h3>

            <div class="stream-meta">
                <div itemprop="author">
                    
                    
                    Mykhaylo Andriluka,
                    
                    
                    
                    Umar Iqbal,
                    
                    
                    
                    <b>Eldar Insafutdinov</b>,
                    
                    
                    
                    Leonid Pishchulin,
                    
                    
                    
                    Anton Milan,
                    
                    
                    
                    Juergen Gall,
                    
                    

                    
                    
                    Bernt Schiele
                    
                    
                </div>
            </div>

            
            <div>
                <i>CVPR, 2018</i>
            </div>
            

            <div class="btn-links">
            







  
    
  


<a class="btn btn-outline-primary my-1 mr-1" href="https://arxiv.org/pdf/1710.10000" target="_blank" rel="noopener">
  pdf
</a>




<a class="btn btn-outline-primary my-1 mr-1" href="https://posetrack.net/" target="_blank" rel="noopener">
  project page
</a>




  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1" href="javascript:toggleblock('andriluka2018cvpr_abstract')">
  abstract
</a>




<button type="button" class="btn btn-outline-primary my-1 mr-1 js-cite-modal"
        data-filename="/publication/andriluka-2018-cvpr/andriluka-2018-cvpr.bib">
  bibtex
</button>













            </div>

            
            
            
            
            
            
            <div class="article-style abstract-block" itemprop="articleBody" id="andriluka2018cvpr_abstract" style="display: none;">
            Existing systems for video-based pose estimation and tracking struggle to perform well on realistic videos with multiple people and often fail to output body-pose trajectories consistent over time. To address this shortcoming this paper introduces PoseTrack which is a new large-scale benchmark for video-based human pose estimation and articulated tracking. Our new benchmark encompasses three tasks focusing on i) single-frame multi-person pose estimation, ii) multi-person pose estimation in videos, and iii) multi-person articulated tracking. To establish the benchmark, we collect, annotate and release a new dataset that features videos with multiple people labeled with person tracks and articulated pose. A public centralized evaluation server is provided to allow the research community to evaluate on a held-out test set. Furthermore, we conduct an extensive experimental study on recent approaches to articulated pose tracking and provide analysis of the  strengths and weaknesses of the state of the art. We envision that the proposed benchmark will stimulate productive research both by providing a large and representative training dataset as well as providing a platform to objectively evaluate and compare the proposed methods.
            </div>
            

        </div>
        </div>
    </div>
</div>
    
        <div class="row">
    
    
        
    
    <div class="col-12 col-lg-3">
        <div class="ml-3">
            
            
            
            <a href="https://arxiv.org/pdf/1612.01465">
                <img src="/publication/insafutdinov-2017-cvpr/featured.png" itemprop="image" class="paper-teaser">
            </a>
            
        </div>
    </div>
    <div class="col-12 col-lg-9">
        <div class="media stream-item" itemscope itemtype="http://schema.org/ScholarlyArticle">
        <div class="media-body">

            <h3 class="article-title mb-0 mt-0" itemprop="name">
            <a href="https://arxiv.org/pdf/1612.01465" itemprop="url">ArtTrack: Articulated Multi-Person Tracking in the Wild</a>
            </h3>

            <div class="stream-meta">
                <div itemprop="author">
                    
                    
                    <b>Eldar Insafutdinov</b>,
                    
                    
                    
                    Mykhaylo Andriluka,
                    
                    
                    
                    Leonid Pishchulin,
                    
                    
                    
                    Siyu Tang,
                    
                    
                    
                    Evgeny Levinkov,
                    
                    
                    
                    Bjoern Andres,
                    
                    

                    
                    
                    Bernt Schiele
                    
                    
                </div>
            </div>

            
            <div>
                <i>CVPR, 2017 (<strong>oral presentation</strong>)</i>
            </div>
            

            <div class="btn-links">
            







  
    
  


<a class="btn btn-outline-primary my-1 mr-1" href="https://arxiv.org/pdf/1612.01465" target="_blank" rel="noopener">
  pdf
</a>




<a class="btn btn-outline-primary my-1 mr-1" href="https://pose.mpi-inf.mpg.de/art-track/" target="_blank" rel="noopener">
  project page
</a>




  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1" href="javascript:toggleblock('insafutdinov2017cvpr_abstract')">
  abstract
</a>




<button type="button" class="btn btn-outline-primary my-1 mr-1 js-cite-modal"
        data-filename="/publication/insafutdinov-2017-cvpr/insafutdinov-2017-cvpr.bib">
  bibtex
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1" href="https://github.com/eldar/pose-tensorflow" target="_blank" rel="noopener">
  code
</a>








  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1" href="https://www.youtube.com/watch?v=TClSwDRIJUQ" target="_blank" rel="noopener">
  video
</a>





            </div>

            
            
            
            
            
            
            <div class="article-style abstract-block" itemprop="articleBody" id="insafutdinov2017cvpr_abstract" style="display: none;">
            In this paper we propose an approach for articulated tracking of multiple people in unconstrained videos. Our starting point is a model that resembles existing architectures for single-frame pose estimation but is substantially faster. We achieve this in two ways: (1) by simplifying and sparsifying the body-part relationship graph and leveraging recent methods for faster inference, and (2) by offloading a substantial share of computation onto a feed-forward convolutional architecture that is able to detect and associate body joints of the same person even in clutter. We use this model to generate proposals for body joint locations and formulate articulated tracking as spatio-temporal grouping of such proposals. This allows to jointly solve the association problem for all people in the scene by propagating evidence from strong detections through time and enforcing constraints that each proposal can be assigned to one person only. We report results on a public “MPII Human Pose” benchmark and on a new “MPII Video Pose” dataset of image sequences with multiple people. We demonstrate that our model achieves state-of-the-art results while using only a fraction of time and is able to leverage temporal information to improve state-of-the-art for crowded scenes
            </div>
            

        </div>
        </div>
    </div>
</div>
    
        <div class="row">
    
    
        
    
    <div class="col-12 col-lg-3">
        <div class="ml-3">
            
            
            
            <a href="https://arxiv.org/pdf/1611.04399">
                <img src="/publication/levinkov-2017-cvpr/featured.png" itemprop="image" class="paper-teaser">
            </a>
            
        </div>
    </div>
    <div class="col-12 col-lg-9">
        <div class="media stream-item" itemscope itemtype="http://schema.org/ScholarlyArticle">
        <div class="media-body">

            <h3 class="article-title mb-0 mt-0" itemprop="name">
            <a href="https://arxiv.org/pdf/1611.04399" itemprop="url">Joint Graph Decomposition and Node Labeling: Problem, Algorithms, Applications</a>
            </h3>

            <div class="stream-meta">
                <div itemprop="author">
                    
                    
                    Evgeny Levinkov,
                    
                    
                    
                    Jonas Uhrig,
                    
                    
                    
                    Siyu Tang,
                    
                    
                    
                    Mohamed Omran,
                    
                    
                    
                    <b>Eldar Insafutdinov</b>,
                    
                    
                    
                    Alexander Kirillov,
                    
                    
                    
                    Carsten Rother,
                    
                    
                    
                    Thomas Brox,
                    
                    
                    
                    Bernt Schiele,
                    
                    

                    
                    
                    Bjoern Andres
                    
                    
                </div>
            </div>

            
            <div>
                <i>CVPR, 2017</i>
            </div>
            

            <div class="btn-links">
            







  
    
  


<a class="btn btn-outline-primary my-1 mr-1" href="https://arxiv.org/pdf/1611.04399" target="_blank" rel="noopener">
  pdf
</a>







  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1" href="javascript:toggleblock('levinkov2017cvpr_abstract')">
  abstract
</a>




<button type="button" class="btn btn-outline-primary my-1 mr-1 js-cite-modal"
        data-filename="/publication/levinkov-2017-cvpr/levinkov-2017-cvpr.bib">
  bibtex
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1" href="https://github.com/bjoern-andres/graph" target="_blank" rel="noopener">
  code
</a>












            </div>

            
            
            
            
            
            
            <div class="article-style abstract-block" itemprop="articleBody" id="levinkov2017cvpr_abstract" style="display: none;">
            We state a combinatorial optimization problem whose feasible solutions define both a decomposition and a node labeling of a given graph.  This problem offers a common mathematical abstraction of seemingly unrelated computer vision tasks, including instance-separating semantic segmentation, articulated human body pose estimation and multiple object tracking. Conceptually, the problem we state generalizes the unconstrained integer quadratic program and the minimum cost lifted multicut problem, both of which are NP-hard. In order to find feasible solutions efficiently, we define two local search algorithms that converge monotonously to a local optimum, offering a feasible solution at any time. To demonstrate their effectiveness in tackling computer vision tasks, we apply these algorithms to instances of the problem that we construct from published data, using published algorithms. We report state-of-the-art application-specific accuracy for the three above-mentioned applications.
            </div>
            

        </div>
        </div>
    </div>
</div>
    
        <div class="row">
    
    
        
    
    <div class="col-12 col-lg-3">
        <div class="ml-3">
            
            
            
            <a href="http://gvv.mpi-inf.mpg.de/projects/EgoCap/content/rhodin2016egocap.pdf">
                <img src="/publication/rhodin-2016-siggraphasia/featured.png" itemprop="image" class="paper-teaser">
            </a>
            
        </div>
    </div>
    <div class="col-12 col-lg-9">
        <div class="media stream-item" itemscope itemtype="http://schema.org/ScholarlyArticle">
        <div class="media-body">

            <h3 class="article-title mb-0 mt-0" itemprop="name">
            <a href="http://gvv.mpi-inf.mpg.de/projects/EgoCap/content/rhodin2016egocap.pdf" itemprop="url">EgoCap: Egocentric Marker-less Motion Capture with Two Fisheye Cameras</a>
            </h3>

            <div class="stream-meta">
                <div itemprop="author">
                    
                    
                    Helge Rhodin,
                    
                    
                    
                    Christian Richardt,
                    
                    
                    
                    Dan Casas,
                    
                    
                    
                    <b>Eldar Insafutdinov</b>,
                    
                    
                    
                    Mohammad Shafiei,
                    
                    
                    
                    Hans-Peter Seidel,
                    
                    
                    
                    Bernt Schiele,
                    
                    

                    
                    
                    Christian Theobalt
                    
                    
                </div>
            </div>

            
            <div>
                <i>SIGGRAPH Asia, 2016</i>
            </div>
            

            <div class="btn-links">
            







  
    
  


<a class="btn btn-outline-primary my-1 mr-1" href="http://gvv.mpi-inf.mpg.de/projects/EgoCap/content/rhodin2016egocap.pdf" target="_blank" rel="noopener">
  pdf
</a>




<a class="btn btn-outline-primary my-1 mr-1" href="http://gvv.mpi-inf.mpg.de/projects/EgoCap/" target="_blank" rel="noopener">
  project page
</a>




  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1" href="javascript:toggleblock('rhodin2016siggraphasia_abstract')">
  abstract
</a>




<button type="button" class="btn btn-outline-primary my-1 mr-1 js-cite-modal"
        data-filename="/publication/rhodin-2016-siggraphasia/rhodin-2016-siggraphasia.bib">
  bibtex
</button>







  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1" href="http://gvv.mpi-inf.mpg.de/projects/EgoCap/content/SA2016-rhodin_web.pptx" target="_blank" rel="noopener">
  slides
</a>



  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1" href="https://www.youtube.com/watch?v=v-4E-jtOet0" target="_blank" rel="noopener">
  video
</a>





            </div>

            
            
            
            
            
            
            <div class="article-style abstract-block" itemprop="articleBody" id="rhodin2016siggraphasia_abstract" style="display: none;">
            Marker-based and marker-less optical skeletal motion-capture methods use an <em>outside-in</em> arrangement of cameras placed around a scene, with viewpoints converging on the center. They often create discomfort with marker suits, and their recording volume is severely restricted and often constrained to indoor scenes with controlled backgrounds.  Alternative suit-based systems use  several  inertial measurement units or an exoskeleton to capture motion with an inside-in setup, i.e. without external sensors. This makes capture independent of a confined volume, but requires substantial, often constraining, and hard to set up body instrumentation. Therefore, we propose a new method for real-time, marker-less, and egocentric motion capture: estimating the full-body skeleton pose from a lightweight stereo pair of fisheye cameras attached to a helmet or virtual reality headset – an optical inside-in method, so to speak. This allows full-body motion capture in general indoor and outdoor scenes, including crowded scenes with many people nearby, which enables reconstruction in larger-scale activities. Our approach combines the strength of a new generative pose estimation framework for fisheye views with a ConvNet-based body-part detector trained on a large new dataset. It is particularly useful in virtual reality to freely roam and interact, while seeing the fully motion-captured virtual body.
            </div>
            

        </div>
        </div>
    </div>
</div>
    
        <div class="row">
    
    
        
    
    <div class="col-12 col-lg-3">
        <div class="ml-3">
            
            
            
            <a href="https://arxiv.org/pdf/1605.03170">
                <img src="/publication/insafutdinov-2016-eccv/featured.png" itemprop="image" class="paper-teaser">
            </a>
            
        </div>
    </div>
    <div class="col-12 col-lg-9">
        <div class="media stream-item" itemscope itemtype="http://schema.org/ScholarlyArticle">
        <div class="media-body">

            <h3 class="article-title mb-0 mt-0" itemprop="name">
            <a href="https://arxiv.org/pdf/1605.03170" itemprop="url">DeeperCut: A Deeper, Stronger, and Faster Multi-Person Pose Estimation Model</a>
            </h3>

            <div class="stream-meta">
                <div itemprop="author">
                    
                    
                    <b>Eldar Insafutdinov</b>,
                    
                    
                    
                    Leonid Pishchulin,
                    
                    
                    
                    Bjoern Andres,
                    
                    
                    
                    Mykhaylo Andriluka,
                    
                    

                    
                    
                    Bernt Schiele
                    
                    
                </div>
            </div>

            
            <div>
                <i>ECCV, 2016</i>
            </div>
            

            <div class="btn-links">
            







  
    
  


<a class="btn btn-outline-primary my-1 mr-1" href="https://arxiv.org/pdf/1605.03170" target="_blank" rel="noopener">
  pdf
</a>




<a class="btn btn-outline-primary my-1 mr-1" href="http://pose.mpi-inf.mpg.de/" target="_blank" rel="noopener">
  project page
</a>




  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1" href="javascript:toggleblock('insafutdinov2016eccv_abstract')">
  abstract
</a>




<button type="button" class="btn btn-outline-primary my-1 mr-1 js-cite-modal"
        data-filename="/publication/insafutdinov-2016-eccv/insafutdinov-2016-eccv.bib">
  bibtex
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1" href="https://github.com/eldar/pose-tensorflow" target="_blank" rel="noopener">
  code
</a>












            </div>

            
            
            
            
            
            
            <div class="article-style abstract-block" itemprop="articleBody" id="insafutdinov2016eccv_abstract" style="display: none;">
            The goal of this paper is to advance the state-of-the-art of articulated pose estimation in scenes with multiple people. To that end we contribute on three fronts. We propose (1) improved body part detectors that generate effective bottom-up proposals for body parts; (2) novel image-conditioned pairwise terms that allow to assemble the proposals into a variable number of consistent body part configurations; and (3) an incremental optimization strategy that explores the search space more efficiently thus leading both to better performance and significant speed-up factors. We evaluate our approach on two single-person and two multi-person pose estimation benchmarks. The proposed approach significantly outperforms best known multi-person pose estimation results while demonstrating competitive performance on the task of single person pose estimation.
            </div>
            

        </div>
        </div>
    </div>
</div>
    
</div>

    </div>
  </section>
  

  
  
  
  <section id="other" class="home-section">
    <div class="container">
      


<div class="row">
  
  <div class="col-12 col-lg-3 section-heading">
    <h1>Other</h1>
    
  </div>
  <div class="col-12 col-lg-9">
    

<h2 id="deeplabcut">DeepLabCut</h2>

<p align="left">
<img src="/img/mouse.gif" style="display:inline;height:210px">
<img src="/img/fly.gif" style="display:inline;height:210px">
<img src="/img/rat-grasp.gif" style="display:inline;height:210px">
</p>

<p><em>Above: courtesy of the <a href="https://vnmurthylab.org/" target="_blank">Murthy</a> (mouse), <a href="http://leventhal.lab.medicine.umich.edu/" target="_blank">Leventhal</a> (rat), and <a href="http://www.axellab.columbia.edu/home.php.html" target="_blank">Axel</a> (fly) labs.</em></p>

<p>My keypoint detection algorithm from the DeeperCut paper and <a href="https://github.com/eldar/pose-tensorflow" target="_blank">its implementation</a> served as the foundation for <a href="http://www.mousemotorlab.org/deeplabcut" target="_blank">DeepLabCut</a>, a toolbox for studying motor behavior of animals in the lab setting developed by neuroscientists at the Universities of Tübingen and Harvard. The toolbox gained significant popularity and is now used in hundreds of labs for a variety of tracking tasks. It is discussed in the corresponding <a href="https://www.nature.com/articles/s41593-018-0209-y.epdf" target="_blank">Nature Neuroscience paper</a> and was covered in the popular media such as <a href="https://news.harvard.edu/gazette/story/2018/08/an-open-source-ai-tool-available-to-study-movement-across-behaviors-and-species/" target="_blank">The Harvard Gazette</a> and <a href="https://www.theatlantic.com/science/archive/2018/07/deeplabcut-tracking-animal-movements/564338" target="_blank">The Atlantic</a>.</p>

<h2 id="multi-person-body-pose-estimation-demo-at-eccv-2016">Multi-person Body Pose Estimation demo at ECCV 2016</h2>

<p>We gave a real-time demonstration of our Multi-person Body Pose Estimation algorithm at <a href="http://www.eccv2016.org/main-conference/" target="_blank">ECCV 2016</a>.</p>

  </div>
  
</div>

    </div>
  </section>
  

  
  
  
  <section id="teaching" class="home-section">
    <div class="container">
      


<div class="row">
  
  <div class="col-12 col-lg-3 section-heading">
    <h1>Teaching</h1>
    
  </div>
  <div class="col-12 col-lg-9">
    <ul>
<li><p><a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/teaching/courses/" target="_blank">Probabilistic Graphical Models and their Applications</a><br />
<em>Saarland University</em><br />
Winter 2015, Winter 2016</p></li>

<li><p>Supplementary Programming Languages (Python)<br />
<em>Bashkir State University</em><br />
Winter, 2013</p></li>
</ul>

  </div>
  
</div>

    </div>
  </section>
  



<div class="container">
  <footer class="site-footer">
  

  <p class="powered-by">
    &copy; 2018 &middot; 
    The <a href="https://github.com/eldar/personal-website/" target="_blank" rel="noopener">source code</a>
    of this website is based on the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

</div>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha512-+NqPlbbtM1QqiK8ZAo4Yrj2c4lNQoGv8P79DPtKzj++l5jnN39rHA/xsqn8zE9l0uSoxaCdrOgFs6yjyfbBxSg==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha256-VsEqElsCHSGmnmHXGQzvoWjWwoznFSZc6hs7ARLRacQ=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
      

      
      
    

    
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
    
    <script src="/js/academic.min.d037ee5294b166a79dec317c58aea9cc.js"></script>

    
    <script src="/js/misc.js"></script>
    

  </body>
</html>


