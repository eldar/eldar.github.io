[{"authors":["Stanislaw Szymanowicz","Eldar Insafutdinov","Chuanxia Zheng","Dylan Campbell","João F. Henriques","Christian Rupprecht","Andrea Vedaldi"],"categories":null,"content":"","date":1717628400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717628400,"objectID":"d333ba7582607238b08a3b77f7fe4f08","permalink":"/publication/szymanowicz-2025-3dv/","publishdate":"2024-06-06T00:00:00+01:00","relpermalink":"/publication/szymanowicz-2025-3dv/","section":"publication","summary":"In this paper, we propose Flash3D, a method for scene reconstruction and novel view synthesis from a single image which is both very generalisable and efficient. For generalisability, we start from a \"foundation\" model for monocular depth estimation and extend it to a full 3D shape and appearance reconstructor. For efficiency, we base this extension on feed-forward Gaussian Splatting. Specifically, we predict a first layer of 3D Gaussians at the predicted depth, and then add additional layers of Gaussians that are offset in space, allowing the model to complete the reconstruction behind occlusions and truncations. Flash3D is very efficient, trainable on a single GPU in a day, and thus accessible to most researchers. It achieves state-of-the-art results when trained and tested on RealEstate10k. When transferred to unseen datasets like NYU it outperforms competitors by a large margin. More impressively, when transferred to KITTI, Flash3D achieves better PSNR than methods trained specifically on that dataset. In some instances, it even outperforms recent methods that use multiple views as input.","tags":null,"title":"Flash3D: Feed-Forward Generalisable 3D Scene Reconstruction from a Single Image","type":"publication"},{"authors":["Eldar Insafutdinov","Dylan Campbell","João F. Henriques","Andrea Vedaldi"],"categories":null,"content":"","date":1664578800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664578800,"objectID":"0e81f5d7629614363c5532f3037fe4c4","permalink":"/publication/insafutdinov-2022-eccv/","publishdate":"2022-10-01T00:00:00+01:00","relpermalink":"/publication/insafutdinov-2022-eccv/","section":"publication","summary":"We present a method for the accurate 3D reconstruction of partly-symmetric objects. We build on the strengths of recent advances in neural reconstruction and rendering such as Neural Radiance Fields (NeRF). A major shortcoming of such approaches is that they fail to reconstruct any part of the object which is not clearly visible in the training image, which is often the case for in-the-wild images and videos. When evidence is lacking, structural priors such as symmetry can be used to complete the missing information. However, exploiting such priors in neural rendering is highly non-trivial: while geometry and non-reflective materials may be symmetric, shadows and reflections from the ambient scene are not symmetric in general. To address this, we apply a soft symmetry constraint to the 3D geometry and material properties, having factored appearance into lighting, albedo colour and reflectivity. We evaluate our method on the recently introduced CO3D dataset, focusing on the car category due to the challenge of reconstructing highly-reflective materials. We show that it can reconstruct unobserved regions with high fidelity and render high-quality novel view images.","tags":null,"title":"SNeS: Learning Probably Symmetric Neural Surfaces from Incomplete Data","type":"publication"},{"authors":["Robert McCraith","Eldar Insafutdinov","Lukas Neumann","Andrea Vedaldi"],"categories":null,"content":"","date":1653260400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653260400,"objectID":"9e44bc4c23befb72651c3c1d19c1943e","permalink":"/publication/mccraith-2022-icra/","publishdate":"2022-05-23T00:00:00+01:00","relpermalink":"/publication/mccraith-2022-icra/","section":"publication","summary":"We present a system for automatic converting of 2D mask object predictions and raw LiDAR point clouds into full 3D bounding boxes of objects. Because the LiDAR point clouds are partial, directly fitting bounding boxes to the point clouds is meaningless. Instead, we suggest that obtaining good results requires sharing information between all objects in the dataset jointly, over multiple frames. We then make three improvements to the baseline. First, we address ambiguities in predicting the object rotations via direct optimization in this space while still backpropagating rotation prediction through the model. Second, we explicitly model outliers and task the network with learning their typical patterns, thus better discounting them. Third, we enforce temporal consistency when video data is available. With these contributions, our method significantly outperforms previous work despite the fact that those methods use significantly more complex pipelines, 3D models and additional human-annotated external sources of prior information.","tags":null,"title":"Lifting 2D Object Locations to 3D by Discounting LiDAR Outliers across Objects and Views","type":"publication"},{"authors":["Verica Lazova","Eldar Insafutdinov","Verica Lazova"],"categories":null,"content":"","date":1543708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543708800,"objectID":"00503dcf129beac4da77a95489548e3f","permalink":"/publication/lazova-2019-3dv/","publishdate":"2018-12-02T00:00:00Z","relpermalink":"/publication/lazova-2019-3dv/","section":"publication","summary":"In this paper we predict a full 3D avatar of a person from a single image. We infer texture and geometry in the UV-space of the SMPL model using an image-to-image translation method. Given partial texture and segmentation layout maps derived from the input view, our model predicts the complete segmentation map, the complete texture map, and a displacement map. The predicted maps can be applied to the SMPL model in order to naturally generalize to novel poses, shapes, and even new clothing. In order to learn our model in a common UV-space, we non-rigidly register the SMPL model to thousands of 3D scans, effectively encoding textures and geometries as images in correspondence. This turns a difficult 3D inference task into a simpler image-to-image translation one. Results on rendered scans of people and images from the DeepFashion dataset demonstrate that our method can reconstruct plausible 3D avatars from a single image. We further use our model to digitally change pose, shape, swap garments between people and edit clothing. To encourage research in this direction we will make the source code available for research purpose.","tags":null,"title":"360-Degree Textures of People in Clothing from a Single Image","type":"publication"},{"authors":["Eldar Insafutdinov","Alexey Dosovitskiy"],"categories":null,"content":"","date":1543708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543708800,"objectID":"6fcac1ac12380fd23333a733bb2bf2aa","permalink":"/publication/insafutdinov-2018-neurips/","publishdate":"2018-12-02T00:00:00Z","relpermalink":"/publication/insafutdinov-2018-neurips/","section":"publication","summary":"We address the problem of learning accurate 3D shape and camera pose from a collection of unlabeled category-specific images. We train a convolutional network to predict both the shape and the pose from a single image by minimizing the reprojection error: given several views of an object, the projections of the predicted shapes to the predicted camera poses should match the provided views. To deal with pose ambiguity, we introduce an ensemble of pose predictors that we then distill it to a single “student” model. To allow for efficient learning of high-fidelity shapes, we represent the shapes by point clouds and devise a formulation allowing for differentiable projection of these. Our experiments show that the distilled ensemble of pose predictors learns to estimate the pose accurately, while the point cloud representation allows to predict detailed shape models.","tags":null,"title":"Unsupervised Learning of Shape and Pose with Differentiable Point Clouds","type":"publication"},{"authors":["Mykhaylo Andriluka","Umar Iqbal","Eldar Insafutdinov","Leonid Pishchulin","Anton Milan","Juergen Gall","Bernt Schiele"],"categories":null,"content":"","date":1527807600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527807600,"objectID":"8e059c1f7df17d4f4057693d4d4097c7","permalink":"/publication/andriluka-2018-cvpr/","publishdate":"2018-06-01T00:00:00+01:00","relpermalink":"/publication/andriluka-2018-cvpr/","section":"publication","summary":"Existing systems for video-based pose estimation and tracking struggle to perform well on realistic videos with multiple people and often fail to output body-pose trajectories consistent over time. To address this shortcoming this paper introduces PoseTrack which is a new large-scale benchmark for video-based human pose estimation and articulated tracking. Our new benchmark encompasses three tasks focusing on i) single-frame multi-person pose estimation, ii) multi-person pose estimation in videos, and iii) multi-person articulated tracking. To establish the benchmark, we collect, annotate and release a new dataset that features videos with multiple people labeled with person tracks and articulated pose. A public centralized evaluation server is provided to allow the research community to evaluate on a held-out test set. Furthermore, we conduct an extensive experimental study on recent approaches to articulated pose tracking and provide analysis of the  strengths and weaknesses of the state of the art. We envision that the proposed benchmark will stimulate productive research both by providing a large and representative training dataset as well as providing a platform to objectively evaluate and compare the proposed methods.","tags":null,"title":"PoseTrack: A Benchmark for Human Pose Estimation and Tracking","type":"publication"},{"authors":["Eldar Insafutdinov","Mykhaylo Andriluka","Leonid Pishchulin","Siyu Tang","Evgeny Levinkov","Bjoern Andres","Bernt Schiele"],"categories":null,"content":"","date":1501023600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501023600,"objectID":"89914ff1d62bcb65dd9683a2e81d41c2","permalink":"/publication/insafutdinov-2017-cvpr/","publishdate":"2017-07-26T00:00:00+01:00","relpermalink":"/publication/insafutdinov-2017-cvpr/","section":"publication","summary":"In this paper we propose an approach for articulated tracking of multiple people in unconstrained videos. Our starting point is a model that resembles existing architectures for single-frame pose estimation but is substantially faster. We achieve this in two ways: (1) by simplifying and sparsifying the body-part relationship graph and leveraging recent methods for faster inference, and (2) by offloading a substantial share of computation onto a feed-forward convolutional architecture that is able to detect and associate body joints of the same person even in clutter. We use this model to generate proposals for body joint locations and formulate articulated tracking as spatio-temporal grouping of such proposals. This allows to jointly solve the association problem for all people in the scene by propagating evidence from strong detections through time and enforcing constraints that each proposal can be assigned to one person only. We report results on a public “MPII Human Pose” benchmark and on a new “MPII Video Pose” dataset of image sequences with multiple people. We demonstrate that our model achieves state-of-the-art results while using only a fraction of time and is able to leverage temporal information to improve state-of-the-art for crowded scenes","tags":null,"title":"ArtTrack: Articulated Multi-Person Tracking in the Wild","type":"publication"},{"authors":["Evgeny Levinkov","Jonas Uhrig","Siyu Tang","Mohamed Omran","Eldar Insafutdinov","Alexander Kirillov","Carsten Rother","Thomas Brox","Bernt Schiele","Bjoern Andres"],"categories":null,"content":"","date":1501023600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501023600,"objectID":"ac1c970b219e68737930fe3f92797423","permalink":"/publication/levinkov-2017-cvpr/","publishdate":"2017-07-26T00:00:00+01:00","relpermalink":"/publication/levinkov-2017-cvpr/","section":"publication","summary":"We state a combinatorial optimization problem whose feasible solutions define both a decomposition and a node labeling of a given graph.  This problem offers a common mathematical abstraction of seemingly unrelated computer vision tasks, including instance-separating semantic segmentation, articulated human body pose estimation and multiple object tracking. Conceptually, the problem we state generalizes the unconstrained integer quadratic program and the minimum cost lifted multicut problem, both of which are NP-hard. In order to find feasible solutions efficiently, we define two local search algorithms that converge monotonously to a local optimum, offering a feasible solution at any time. To demonstrate their effectiveness in tackling computer vision tasks, we apply these algorithms to instances of the problem that we construct from published data, using published algorithms. We report state-of-the-art application-specific accuracy for the three above-mentioned applications.","tags":null,"title":"Joint Graph Decomposition and Node Labeling: Problem, Algorithms, Applications","type":"publication"},{"authors":["Helge Rhodin","Christian Richardt","Dan Casas","Eldar Insafutdinov","Mohammad Shafiei","Hans-Peter Seidel","Bernt Schiele","Christian Theobalt"],"categories":null,"content":"","date":1480896000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480896000,"objectID":"359087bb807ca65b79a91206a58ad4da","permalink":"/publication/rhodin-2016-siggraphasia/","publishdate":"2016-12-05T00:00:00Z","relpermalink":"/publication/rhodin-2016-siggraphasia/","section":"publication","summary":"Marker-based and marker-less optical skeletal motion-capture methods use an *outside-in* arrangement of cameras placed around a scene, with viewpoints converging on the center. They often create discomfort with marker suits, and their recording volume is severely restricted and often constrained to indoor scenes with controlled backgrounds.  Alternative suit-based systems use  several  inertial measurement units or an exoskeleton to capture motion with an inside-in setup, i.e. without external sensors. This makes capture independent of a confined volume, but requires substantial, often constraining, and hard to set up body instrumentation. Therefore, we propose a new method for real-time, marker-less, and egocentric motion capture: estimating the full-body skeleton pose from a lightweight stereo pair of fisheye cameras attached to a helmet or virtual reality headset – an optical inside-in method, so to speak. This allows full-body motion capture in general indoor and outdoor scenes, including crowded scenes with many people nearby, which enables reconstruction in larger-scale activities. Our approach combines the strength of a new generative pose estimation framework for fisheye views with a ConvNet-based body-part detector trained on a large new dataset. It is particularly useful in virtual reality to freely roam and interact, while seeing the fully motion-captured virtual body.","tags":null,"title":"EgoCap: Egocentric Marker-less Motion Capture with Two Fisheye Cameras","type":"publication"},{"authors":["Eldar Insafutdinov","Leonid Pishchulin","Bjoern Andres","Mykhaylo Andriluka","Bernt Schiele"],"categories":null,"content":"","date":1475881200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1475881200,"objectID":"37af962769c21035844c516822043195","permalink":"/publication/insafutdinov-2016-eccv/","publishdate":"2016-10-08T00:00:00+01:00","relpermalink":"/publication/insafutdinov-2016-eccv/","section":"publication","summary":"The goal of this paper is to advance the state-of-the-art of articulated pose estimation in scenes with multiple people. To that end we contribute on three fronts. We propose (1) improved body part detectors that generate effective bottom-up proposals for body parts; (2) novel image-conditioned pairwise terms that allow to assemble the proposals into a variable number of consistent body part configurations; and (3) an incremental optimization strategy that explores the search space more efficiently thus leading both to better performance and significant speed-up factors. We evaluate our approach on two single-person and two multi-person pose estimation benchmarks. The proposed approach significantly outperforms best known multi-person pose estimation results while demonstrating competitive performance on the task of single person pose estimation.","tags":null,"title":"DeeperCut: A Deeper, Stronger, and Faster Multi-Person Pose Estimation Model","type":"publication"},{"authors":["Leonid Pishchulin","Eldar Insafutdinov","Siyu Tang","Bjoern Andres","Mykhaylo Andriluka","Peter Gehler","Bernt Schiele"],"categories":null,"content":"","date":1464735600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464735600,"objectID":"014bb87c54623906f43508e72cb7c0b7","permalink":"/publication/pishchulin-2016-cvpr/","publishdate":"2016-06-01T00:00:00+01:00","relpermalink":"/publication/pishchulin-2016-cvpr/","section":"publication","summary":"This paper considers the task of articulated human pose estimation of multiple people in real world images. We propose an approach that jointly solves the tasks of detection and pose estimation:  it infers the number of persons in a scene, identifies occluded body parts, and disambiguates body parts between people in close proximity of each other. This joint formulation is in contrast to previous strategies, that address the problem by first detecting people and subsequently estimating their body pose. We propose a partitioning and labeling formulation of a set of body-part hypotheses generated with CNN-based part detectors. Our formulation, an instance of an integer linear program, implicitly performs non-maximum suppression on the set of part candidates and groups them to form configurations of body parts respecting geometric and appearance constraints. Experiments on four different datasets demonstrate state-of-the-art results for both single person and multi person pose estimation.","tags":null,"title":"DeepCut: Joint Subset Partition and Labeling for Multi Person Pose Estimation","type":"publication"}]